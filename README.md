# Medpace inSitE - Snowflake Optimization

This repository contains the analysis and implementation guide for optimizing Medpace's **inSitE** (Informatics Site Engine) application by moving data processing from R/Shiny to Snowflake.

## üìÅ Repository Contents

### üìÑ Documentation

**[Medpace_inSitE_Optimization_Summary.md](./Medpace_inSitE_Optimization_Summary.md)**
- Executive summary with performance improvements
- Current architecture bottlenecks with real code examples
- Before/after comparisons showing code simplification
- Dynamic Tables vs SQL Functions decision framework
- Implementation timeline and discussion questions

**[Cortex_REST_API_Optimization.md](./Cortex_REST_API_Optimization.md)**
- Optimize Cortex LLM calls to eliminate warehouse costs
- Before/after code comparison for cluster interpretation
- Switch from ODBC SQL approach to direct REST API
- Enhanced prompt for better AI responses

### üîß Implementation

**[Medpace_inSitE_Snowflake_Implementation.ipynb](./Medpace_inSitE_Snowflake_Implementation.ipynb)**
- Production-ready Snowflake notebook with 8 implementation steps
- Dynamic Tables for core metrics (auto-refreshing, <1 second queries)
- SQL Functions for user-specific operations
- Stored Procedures for ML models and Monte Carlo simulations
- Complete monitoring and maintenance queries
- R code examples for integration

### üìä Reference Data

**[app (2).R](./app%20(2).R)**
- Original R Shiny application (3,808 lines)
- Shows current data processing approach
- Identifies specific bottlenecks addressed by optimization

**Table Statistics:**
- `SnowflakeQueryResults.xlsx - gb.csv` - Actual table sizes from Medpace Snowflake
- `SnowflakeQueryResults.xlsx - rows added.csv` - Table row counts and update frequency

---

## üéØ Executive Summary

### Current State
- **16 million rows (730 MB)** loaded into R per analysis
- **2-3 GB memory** per user session
- **3-6 minutes** per analysis
- **5-10 concurrent users** max capacity

### After Optimization
- **<5 MB** data transfer per query
- **100-200 MB memory** per session
- **15-25 seconds** per analysis
- **50-100+ concurrent users** capacity

### Performance Improvements

| Component | Current | After Optimization | Improvement |
|-----------|---------|-------------------|-------------|
| **Data Collation** | 30-60 sec | <1 sec (Dynamic Table) | **30-60x faster** |
| **Monte Carlo (10K)** | 2-5 min | 5-10 sec | **20-30x faster** |
| **Full Analysis** | 3-6 min | 15-25 sec | **12-18x faster** |
| **Memory Usage** | 2-3 GB | 100-200 MB | **95% reduction** |

---

## üèóÔ∏è Architecture Overview

### Recommended Hybrid Approach

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SNOWFLAKE                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                     ‚îÇ
‚îÇ  Dynamic Tables (90% of queries)   ‚îÇ
‚îÇ  ‚îî‚îÄ site_metrics_base              ‚îÇ
‚îÇ     ‚Ä¢ TARGET_LAG = '1 hour'        ‚îÇ
‚îÇ     ‚Ä¢ REFRESH_MODE = 'INCREMENTAL' ‚îÇ
‚îÇ     ‚Ä¢ Query time: <1 second ‚úì      ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  SQL Functions (10% of queries)    ‚îÇ
‚îÇ  ‚îî‚îÄ User-specific filters          ‚îÇ
‚îÇ     ‚Ä¢ Custom study codes           ‚îÇ
‚îÇ     ‚Ä¢ Query time: 1-2 seconds      ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Stored Procedures (ML/complex)    ‚îÇ
‚îÇ  ‚îî‚îÄ Monte Carlo simulations        ‚îÇ
‚îÇ  ‚îî‚îÄ K-means clustering             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì <5 MB transfer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      R SHINY APP                    ‚îÇ
‚îÇ  Only UI and visualization          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start

### 1. Review Documentation
Start with [`Medpace_inSitE_Optimization_Summary.md`](./Medpace_inSitE_Optimization_Summary.md) to understand:
- Why the current architecture is inefficient
- What changes are needed
- Expected benefits

### 2. Implement in Snowflake
Use [`Medpace_inSitE_Snowflake_Implementation.ipynb`](./Medpace_inSitE_Snowflake_Implementation.ipynb):
- Import into Snowflake Notebooks and run cell-by-cell
- Each section has detailed markdown explanations
- Includes testing and monitoring queries

### 3. Update R Application
- Replace data processing with simple queries to Dynamic Tables
- Add function calls for user-specific operations
- Optimize Cortex LLM calls using REST API (see [`Cortex_REST_API_Optimization.md`](./Cortex_REST_API_Optimization.md))
- See R code examples at the end of implementation notebook

---

## üìã Implementation Checklist

### Phase 1: Setup
- [ ] Enable change tracking on base tables
- [ ] Create `CLINOPS_TRANSFORM_WH` warehouse
- [ ] Create `site_metrics_base` Dynamic Table
- [ ] Monitor initial refresh

### Phase 2: Testing
- [ ] Test query performance (<1 second?)
- [ ] Verify data accuracy
- [ ] Monitor refresh performance
- [ ] Adjust `TARGET_LAG` if needed

### Phase 3: Functions & Procedures
- [ ] Create SQL Functions for custom filters
- [ ] Create Stored Procedure for Monte Carlo
- [ ] Test from R application

### Phase 4: Production
- [ ] Update R app to query new objects
- [ ] Optimize Cortex LLM calls (REST API vs ODBC)
- [ ] Parallel testing (old vs new)
- [ ] User acceptance testing
- [ ] Production deployment

---

---

## üîç Key Decisions

### Dynamic Tables vs SQL Functions

**Use Dynamic Tables for:**
- ‚úÖ Site performance metrics (queried frequently)
- ‚úÖ Core aggregations (changes predictably)
- ‚úÖ Shared across multiple users
- ‚úÖ Heavy joins and aggregations

**Use SQL Functions for:**
- ‚úÖ User-entered custom study codes
- ‚úÖ Dynamic filtering (parameters change per query)
- ‚úÖ Always need real-time data

### Setting TARGET_LAG

| Data Type | Update Frequency | Recommended TARGET_LAG |
|-----------|------------------|------------------------|
| Site performance | Daily | `1-2 hours` |
| Study relationships | Weekly | `12-24 hours` |
| Historical data | Monthly | `1-2 days` |

**Monitor and adjust:**
```sql
SELECT 
    target_lag_sec / 60 as target_minutes,
    actual_lag_sec / 60 as actual_minutes
FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLES('site_metrics_base'));
```

---

## üìû Support

### Questions to Discuss
1. **Data Freshness:** Can site metrics be 1-2 hours old?
2. **User Concurrency:** How many simultaneous users?
3. **Warehouse Sizing:** What size for refreshes?
4. **Resource Planning:** Expected concurrent user load?

### Next Steps
- Schedule technical workshop with Snowflake team
- Run proof of concept (Phase 1)
- Measure actual performance improvements
- Make go/no-go decision based on results

---

## üìö Additional Resources

- [Snowflake Dynamic Tables Documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)
- [Snowflake Stored Procedures](https://docs.snowflake.com/en/sql-reference/stored-procedures)
- [Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/index)

---

## üè∑Ô∏è Version History

**v1.0** (October 2025)
- Initial analysis and recommendations
- Production-ready implementation notebook
- Complete before/after documentation

---

## üë• Contributors

**Snowflake Solution Engineering Team**
- Architecture design and recommendations
- Implementation notebook development
- Performance analysis

**Medpace Team**
- Original R/Shiny application
- Requirements and use cases
- Data environment details

---

**License:** Internal Medpace/Snowflake collaboration  
**Status:** Ready for implementation

